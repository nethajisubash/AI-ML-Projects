{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nethajisubash/AI-ML-Projects/blob/main/Week9-Jan22-LinAlg2/PreClass_Linear_Algebra_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56eaebb1",
      "metadata": {
        "id": "56eaebb1"
      },
      "source": [
        "## Simple Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40762c75",
      "metadata": {
        "id": "40762c75"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets as datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q8z_up3GtijB",
      "metadata": {
        "id": "q8z_up3GtijB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q7IT5ikLVTlQ",
      "metadata": {
        "id": "Q7IT5ikLVTlQ"
      },
      "outputs": [],
      "source": [
        "boston_data_path = \"/content/drive/MyDrive/AIConsulting/IK_Arthan_Aujan_Innosential/INTERVIEW KICKSTART/Final Notebooks-make changes here/datasets/Boston.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeG8JuWdRVg2",
      "metadata": {
        "id": "aeG8JuWdRVg2"
      },
      "outputs": [],
      "source": [
        "# reading the dataset\n",
        "df = pd.read_csv(boston_data_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5979cdc",
      "metadata": {
        "id": "b5979cdc"
      },
      "outputs": [],
      "source": [
        "# checking the chape of dataset\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9afHgGClSYCr",
      "metadata": {
        "id": "9afHgGClSYCr"
      },
      "outputs": [],
      "source": [
        "Y = df['MEDV']\n",
        "X = df[['RM']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6f3503",
      "metadata": {
        "id": "7b6f3503"
      },
      "outputs": [],
      "source": [
        "# visualizing the data\n",
        "plt.scatter(X, Y)\n",
        "plt.xlabel('RM')\n",
        "plt.ylabel('MEDV')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8102ac",
      "metadata": {
        "id": "5b8102ac"
      },
      "outputs": [],
      "source": [
        "# splitting training and testing data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.75 , random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d74c88",
      "metadata": {
        "id": "38d74c88"
      },
      "outputs": [],
      "source": [
        "# applying Linear Regression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5defc4b",
      "metadata": {
        "id": "e5defc4b"
      },
      "outputs": [],
      "source": [
        "# printing the coefficients\n",
        "# ypred = a + b * x\n",
        "print('a = ', model.intercept_)\n",
        "print('b = ', model.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599deb9f",
      "metadata": {
        "id": "599deb9f"
      },
      "outputs": [],
      "source": [
        "# Making predictions on the test set\n",
        "Y_pred = model.predict(X_test)\n",
        "Y_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "865267e2",
      "metadata": {
        "id": "865267e2"
      },
      "outputs": [],
      "source": [
        "# visualizing the predictions\n",
        "plt.plot([x for x in range(Y_test.shape[0])][:75], Y_test[:75], color = \"blue\", linewidth = 1, linestyle = \"-\")\n",
        "plt.plot([x for x in range(Y_test.shape[0])][:75], Y_pred[:75], color = \"red\",  linewidth = 1, linestyle = \"-.\")\n",
        "plt.title('Actual value vs Predicted value')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Sales')\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11edfb7a",
      "metadata": {
        "id": "11edfb7a"
      },
      "outputs": [],
      "source": [
        "# visualizing the error terms\n",
        "plt.plot([x for x in range(Y_test.shape[0])], Y_test - Y_pred, color = \"red\", linewidth = 1, linestyle = \"-\")\n",
        "plt.title('Errors')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Sales')\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a1dde4",
      "metadata": {
        "id": "26a1dde4"
      },
      "outputs": [],
      "source": [
        "# mean squared error computation\n",
        "mse = mean_squared_error(Y_test, Y_pred)\n",
        "print(mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "035e2a00",
      "metadata": {
        "id": "035e2a00"
      },
      "outputs": [],
      "source": [
        "# r2 value computation\n",
        "r2 = r2_score(Y_test, Y_pred)\n",
        "print(r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ada7bac",
      "metadata": {
        "id": "9ada7bac"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_test, Y_test, c = 'blue')\n",
        "plt.plot(X_test, Y_pred, c = 'red')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb1d5ac",
      "metadata": {
        "id": "dbb1d5ac"
      },
      "source": [
        "# Multiple Linear Regression using SK-LEARN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe723652",
      "metadata": {
        "id": "fe723652"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa68bfc5",
      "metadata": {
        "id": "aa68bfc5"
      },
      "outputs": [],
      "source": [
        "# reading the dataset\n",
        "df = pd.read_csv(boston_data_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XfW_soRiaqJE",
      "metadata": {
        "id": "XfW_soRiaqJE"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HgtJudlXXHGO",
      "metadata": {
        "id": "HgtJudlXXHGO"
      },
      "outputs": [],
      "source": [
        "Y = df['MEDV']\n",
        "X = df[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UqJWoGkUdGDj",
      "metadata": {
        "id": "UqJWoGkUdGDj"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d9f9eee",
      "metadata": {
        "id": "3d9f9eee"
      },
      "outputs": [],
      "source": [
        "# visualizing the data\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(df.corr(),annot=True,cmap=\"YlGnBu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56cead77",
      "metadata": {
        "id": "56cead77"
      },
      "outputs": [],
      "source": [
        "# splitting variables and generating training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.33, random_state = 101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baac1413",
      "metadata": {
        "id": "baac1413"
      },
      "outputs": [],
      "source": [
        "# Linear Regression Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2859d5",
      "metadata": {
        "id": "5e2859d5"
      },
      "outputs": [],
      "source": [
        "# Making Predictions\n",
        "model.predict(X_test)\n",
        "pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa8878c",
      "metadata": {
        "id": "aaa8878c"
      },
      "outputs": [],
      "source": [
        "# Evaluating Model's Performance\n",
        "print('Mean Absolute Error:', mean_absolute_error(y_test, pred))\n",
        "print('Mean Squared Error:', mean_squared_error(y_test, pred))\n",
        "print('Mean Root Squared Error:', np.sqrt(mean_squared_error(y_test, pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9EmYeMJdcEaw",
      "metadata": {
        "id": "9EmYeMJdcEaw"
      },
      "source": [
        "#Multiple Linear Regression using STATSMODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N3nm-vb-cWuv",
      "metadata": {
        "id": "N3nm-vb-cWuv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import libraries for data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import libraries for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.graphics.gofplots import ProbPlot\n",
        "\n",
        "# Import libraries for building linear regression model using statsmodel\n",
        "from statsmodels.formula.api import ols\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Importing Linear Regression from sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Import library for preparing data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import library for data preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kYlcFsBLdDaQ",
      "metadata": {
        "id": "kYlcFsBLdDaQ"
      },
      "outputs": [],
      "source": [
        "# reading the dataset\n",
        "df = pd.read_csv(boston_data_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XVIey4HggcZ_",
      "metadata": {
        "id": "XVIey4HggcZ_"
      },
      "source": [
        "**Observations:**\n",
        "* The price of the house indicated by the variable MEDV is the target variable and the rest are the independent variables based on which we will predict house price.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tw65RH4_dS_m",
      "metadata": {
        "id": "Tw65RH4_dS_m"
      },
      "outputs": [],
      "source": [
        "\n",
        "#### **Get information about the dataset using the info() method**\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12mSFqVFdWyu",
      "metadata": {
        "id": "12mSFqVFdWyu"
      },
      "source": [
        "**Observations:**\n",
        "* There are a total of 506 non-null observations in each of the columns. This indicates that there are no missing values in the data.\n",
        "\n",
        "* Every column in this dataset is numeric in nature.\n",
        "\n",
        "## **Exploratory Data Analysis**\n",
        "\n",
        "#### **Let's now check the summary statistics of this dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WvaXAS9NdWP1",
      "metadata": {
        "id": "WvaXAS9NdWP1"
      },
      "outputs": [],
      "source": [
        "df.describe().transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q_kBPWLXdth8",
      "metadata": {
        "id": "Q_kBPWLXdth8"
      },
      "source": [
        "### **Let's check the correlation using the heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Izk7ju_RdvKq",
      "metadata": {
        "id": "Izk7ju_RdvKq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (12, 8))\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap = True)\n",
        "sns.heatmap(df.corr(), annot = True, fmt = '.2f', cmap = cmap )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O4FFzb0RdyRj",
      "metadata": {
        "id": "O4FFzb0RdyRj"
      },
      "source": [
        "\n",
        "## **Model Building - Approach**\n",
        "\n",
        "1. Data preparation\n",
        "2. Partition the data into train and test set\n",
        "3. Build model on the train data\n",
        "4. Cross-validating the model\n",
        "5. Test the data on test set\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Split the dataset**\n",
        "Let's split the data into the dependent and independent variables and further split it into train and test set in a ratio of 70:30 for train and test set.\n",
        "\n",
        "# Separate the dependent and indepedent variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bn9n_c5Dd5HS",
      "metadata": {
        "id": "bn9n_c5Dd5HS"
      },
      "outputs": [],
      "source": [
        "Y = df['MEDV']\n",
        "X = df.drop(columns = {'MEDV'})\n",
        "\n",
        "# Add the intercept term\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Splitting the data in 70:30 ratio of train to test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.30 , random_state = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "El37-mU9ec9L",
      "metadata": {
        "id": "El37-mU9ec9L"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Creating linear regression model using statsmodels OLS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v0V_I9C9esKs",
      "metadata": {
        "id": "v0V_I9C9esKs"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model1 = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "# Get the model summary\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "saaWuYsNes64",
      "metadata": {
        "id": "saaWuYsNes64"
      },
      "source": [
        "\n",
        "\n",
        "- We can see that the `R-squared` for the model is `0.71`.\n",
        "- Not all the variables are statistically significant to predict the outcome variable. To check which are statistically significant or have predictive power to predict the target variable, we need to check the `p-value` against all the independent variables.\n",
        "\n",
        "**Interpreting the Regression Results:**\n",
        "\n",
        "1. **Adjusted. R-squared**: It reflects the fit of the model.\n",
        "    - R-squared values range from 0 to 1, where a higher value generally indicates a better fit, assuming certain conditions are met.\n",
        "    - In our case, the value for Adj. R-squared is **0.697**\n",
        "\n",
        "2. **coeff**: It represents the change in the output Y due to a change of one unit in the variable (everything else held constant).\n",
        "3. **std err**: It reflects the level of accuracy of the coefficients.\n",
        "    - The lower it is, the more accurate the coefficients are.\n",
        "4. **P >|t|**: It is p-value.\n",
        "   \n",
        "   * Pr(>|t|): For each independent feature there is a null hypothesis and an  alternate hypothesis\n",
        "\n",
        "    Ho: Independent feature is not significant\n",
        "   \n",
        "    Ha: Independent feature is significant\n",
        "    \n",
        "   * A p-value of less than 0.05 is considered to be statistically significant.\n",
        "\n",
        "   \n",
        "5. **Confidence Interval**: It represents the range in which our coefficients are likely to fall (with a likelihood of 95%).\n",
        "\n",
        "\n",
        "\n",
        "* Both the **R-squared and Adjusted R-squared of the model are around 71%**. This is a clear indication that we have been able to create a good model that can explain variance in the house prices for up to 71%.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mxpo3hwFcCgc",
      "metadata": {
        "id": "mxpo3hwFcCgc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#### **Checking the performance of the model on the train and test data set**\n",
        "\n",
        "# RMSE\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((targets - predictions) ** 2).mean())\n",
        "\n",
        "\n",
        "# MAPE\n",
        "def mape(predictions, targets):\n",
        "    return np.mean(np.abs((targets - predictions)) / targets) * 100\n",
        "\n",
        "\n",
        "# MAE\n",
        "def mae(predictions, targets):\n",
        "    return np.mean(np.abs((targets - predictions)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6dPvoxocxsB",
      "metadata": {
        "id": "b6dPvoxocxsB"
      },
      "outputs": [],
      "source": [
        "# Model Performance on test and train data\n",
        "def model_pref(olsmodel, x_train, x_test, y_train, y_test):\n",
        "\n",
        "    # Insample Prediction\n",
        "    y_pred_train = olsmodel.predict(x_train)\n",
        "    y_observed_train = y_train\n",
        "\n",
        "    # Prediction on test data\n",
        "    y_pred_test = olsmodel.predict(x_test)\n",
        "    y_observed_test = y_test\n",
        "\n",
        "    print(\n",
        "        pd.DataFrame(\n",
        "            {\n",
        "                \"Data\": [\"Train\", \"Test\"],\n",
        "                \"RMSE\": [\n",
        "                    rmse(y_pred_train, y_observed_train),\n",
        "                    rmse(y_pred_test, y_observed_test),\n",
        "                ],\n",
        "                \"MAE\": [\n",
        "                    mae(y_pred_train, y_observed_train),\n",
        "                    mae(y_pred_test, y_observed_test),\n",
        "                ],\n",
        "                \"MAPE\": [\n",
        "                    mape(y_pred_train, y_observed_train),\n",
        "                    mape(y_pred_test, y_observed_test),\n",
        "                ],\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "# Checking model performance\n",
        "model_pref(model1, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TLHOwRddcEna",
      "metadata": {
        "id": "TLHOwRddcEna"
      },
      "source": [
        "## Forward Feature Selection using SequentialFeatureSelector\n",
        "\n",
        "**Why should we do feature selection?**\n",
        "\n",
        "- Reduces dimensionality\n",
        "- Discards deceptive features (Deceptive features appear to aid learning on the training set, but impair generalization)\n",
        "- Speeds training/testing\n",
        "\n",
        "\n",
        "**How does forward feature selection work?**\n",
        "\n",
        "* It starts with an empty model and adds variables one by one.\n",
        "* In each forward step, you add the one variable that gives the highest improvement to your model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KCz7xH1dcqeK",
      "metadata": {
        "id": "KCz7xH1dcqeK"
      },
      "outputs": [],
      "source": [
        "# reading the dataset\n",
        "df = pd.read_csv(boston_data_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qh3n6HOSc7ca",
      "metadata": {
        "id": "qh3n6HOSc7ca"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['MEDV'])\n",
        "Y = df[['MEDV']]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.75 , random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w_gSb000sVLS",
      "metadata": {
        "id": "w_gSb000sVLS"
      },
      "outputs": [],
      "source": [
        "!pip install mlxtend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rv273YJMsbIS",
      "metadata": {
        "id": "Rv273YJMsbIS"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import sys\n",
        "sys.modules['sklearn.externals.joblib'] = joblib\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "\n",
        "\n",
        "reg = LinearRegression()\n",
        "\n",
        "# Build step forward feature selection\n",
        "sfs = SFS(\n",
        "    reg,\n",
        "    k_features=X_train.shape[1],\n",
        "    forward=True,  # k_features denotes the number of features to select\n",
        "    floating=False,\n",
        "    scoring=\"r2\",\n",
        "    verbose=2,\n",
        "    cv=5,\n",
        ")\n",
        "\n",
        "# Perform SFFS\n",
        "sfs = sfs.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KEHN6Z-AuKXt",
      "metadata": {
        "id": "KEHN6Z-AuKXt"
      },
      "outputs": [],
      "source": [
        "# to plot the performance with addition of each feature\n",
        "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
        "\n",
        "fig1 = plot_sfs(sfs.get_metric_dict(), kind=\"std_err\")\n",
        "plt.title(\"Sequential Forward Selection (w. StdErr)\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WgJdxfMPubdn",
      "metadata": {
        "id": "WgJdxfMPubdn"
      },
      "source": [
        "- We can see that performance increases till the 5th feature and then slowly becomes constant, and then drops after the 10th feature is added.\n",
        "- The decision to choose the *k_features* now depends on the adjusted $R^2$ vs the complexity of the model.\n",
        "    - With 5 features, we are getting an adjusted $R^2$ of 0.7223757171836604\n",
        "    - With 9 features, we are getting an adjusted $R^2$ of 0.7315777117242518.\n",
        "    - With 10 features, we are getting an adjusted $R^2$ of 0.7307239154712025.\n",
        "- The increase in adjusted $R^2$ is not very significant as we are getting the same values with a less complex model.\n",
        "- So we'll use 5 features only to build our model, but you can experiment by taking a different number.\n",
        "- Number of features chosen will also depend on the business context and use case of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c895753e",
      "metadata": {
        "id": "c895753e"
      },
      "source": [
        "###Introduction to PCA\n",
        "PCA stands for Principal Component Analysis, and it is a widely used technique in data analysis and machine learning. At its core, PCA is a way to reduce the complexity of high-dimensional data by identifying the most important patterns and trends in the data.\n",
        "\n",
        "Imagine you have a dataset with many variables, such as age, height, weight, income, and education level, and you want to understand how these variables are related to each other. PCA can help you by finding the underlying structure of the data and identifying the key factors that explain most of the variation in the data.\n",
        "\n",
        "To do this, PCA uses linear algebra to transform the data into a new coordinate system that captures the most important information in the data. The new coordinate system is called the principal components, and each principal component is a linear combination of the original variables.\n",
        "\n",
        "By examining the principal components, you can identify the most important patterns in the data and understand how different variables contribute to these patterns. You can also use the principal components to visualize the data in a lower-dimensional space, which can help you identify clusters or groups of similar data points.\n",
        "\n",
        "Overall, PCA is a powerful tool for exploring and analyzing complex datasets, and it can be applied to a wide range of fields, including biology, economics, psychology, and computer science."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391f4cb9",
      "metadata": {
        "id": "391f4cb9"
      },
      "source": [
        "### PCA Theory\n",
        "PCA is based on the concept of linear algebra, specifically the eigenvalue decomposition of a covariance matrix. In simple terms, the covariance matrix is a measure of the linear relationship between pairs of variables. Note that PCA is applied on centered data.\n",
        "\n",
        "Let X be an n x p matrix representing the centered data, where n is the number of observations and p is the number of variables. The covariance matrix of X is given by:\n",
        "\n",
        "C = (1/n) * X^T * X\n",
        "\n",
        "where ^T denotes the transpose of a matrix. The covariance matrix is a symmetric positive semi-definite matrix, which means that it has p real eigenvalues and p orthogonal eigenvectors.\n",
        "\n",
        "The eigendecomposition of C is given by:\n",
        "\n",
        "C = V * Lambda * V^T\n",
        "\n",
        "where V is a p x p matrix whose columns are the eigenvectors of C, and Lambda is a diagonal matrix whose entries are the corresponding eigenvalues.\n",
        "\n",
        "The eigenvectors in V are sorted in descending order according to their corresponding eigenvalues in Lambda. The first principal component is the linear combination of the variables that corresponds to the eigenvector with the largest eigenvalue. The second principal component is the linear combination that corresponds to the eigenvector with the second largest eigenvalue, and so on.\n",
        "\n",
        "To compute the principal components of the data, we multiply the centered data matrix X by the matrix of eigenvectors V:\n",
        "\n",
        "Y = X * V\n",
        "\n",
        "where Y is the matrix of principal components. Each column of Y represents a principal component, and each row represents an observation.\n",
        "\n",
        "The proportion of variance explained by each principal component is given by its corresponding eigenvalue divided by the sum of all eigenvalues:\n",
        "\n",
        "prop_i = lambda_i / (sum(lambda))\n",
        "\n",
        "where prop_i is the proportion of variance explained by the i-th principal component.\n",
        "\n",
        "PCA can be used for data compression by selecting the top k principal components that explain the most variance in the data. The compressed data can be reconstructed by multiplying the matrix of selected principal components by the transpose of the matrix of eigenvectors:\n",
        "\n",
        "X_hat = Y_k * V^T_k\n",
        "\n",
        "where Y_k is the matrix of the k selected principal components, and V_k is the matrix of the corresponding k eigenvectors. X_hat is the reconstructed data, which should be close to the original data X."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b45f6a42",
      "metadata": {
        "id": "b45f6a42"
      },
      "source": [
        "### Data Preparation for PCA\n",
        "Suppose we have a dataset with n observations and p variables. Before applying PCA, we need to perform the following data preparation steps:\n",
        "\n",
        "1. Standardization: PCA is sensitive to the scale of the variables, so we need to standardize the data to have zero mean and unit variance. This can be done by subtracting the mean of each variable and dividing by its standard deviation:\n",
        "X_standardized = (X - mean(X)) / std(X) (where X is the original data matrix, mean(X) is the mean vector of X, and std(X) is the standard deviation vector of X)\n",
        "\n",
        "2. Missing value imputation: If the dataset has missing values, we need to impute them before applying PCA. There are different imputation methods that can be used, such as mean imputation, regression imputation, or multiple imputation.\n",
        "\n",
        "3. Outlier detection: Outliers can affect the results of PCA, so it is important to detect and handle them before applying PCA. One way to detect outliers is by computing the Mahalanobis distance of each observation from the mean of the data. Observations with a large Mahalanobis distance are considered outliers.\n",
        "\n",
        "4. Variable selection: If the dataset has a large number of variables, it may be necessary to perform variable selection before applying PCA. This can be done using various methods, such as correlation analysis, mutual information, or feature importance scores.\n",
        "\n",
        "After performing these data preparation steps, we can apply PCA to the standardized data X_standardized to obtain the principal components and perform exploratory data analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40b67beb",
      "metadata": {
        "id": "40b67beb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afcc346e",
      "metadata": {
        "id": "afcc346e"
      },
      "outputs": [],
      "source": [
        "# reading the dataset\n",
        "df = pd.read_csv(boston_data_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d1d1b1",
      "metadata": {
        "id": "60d1d1b1"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['MEDV'])\n",
        "y = df[['MEDV']]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size = 0.75 , random_state = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c89a65b7",
      "metadata": {
        "id": "c89a65b7"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "Xst = scaler.fit_transform(X)\n",
        "xDf = pd.DataFrame(data = Xst, columns = X.columns)\n",
        "xDf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcea714b",
      "metadata": {
        "id": "bcea714b"
      },
      "source": [
        "### Implementation of PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a6eeca9",
      "metadata": {
        "id": "3a6eeca9"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components = None)\n",
        "xDf_PCA = pca.fit(xDf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "115db6e8",
      "metadata": {
        "id": "115db6e8"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x = [i + 1 for i in range(len(xDf_PCA.explained_variance_ratio_))],\n",
        "            y = xDf_PCA.explained_variance_ratio_,\n",
        "            s = 200, alpha = 0.75, c = 'red', edgecolor = 'm')\n",
        "plt.grid(True)\n",
        "plt.title(\"Explained variance ratio of the fitted principal component vector\")\n",
        "plt.xlabel(\"Principal components\")\n",
        "plt.xticks([i+1 for i in range(len(xDf_PCA.explained_variance_ratio_))])\n",
        "plt.yticks()\n",
        "plt.ylabel(\"Variance ratio\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SLqANsWKhEaj",
      "metadata": {
        "id": "SLqANsWKhEaj"
      },
      "outputs": [],
      "source": [
        "explained_variance_ratio=xDf_PCA.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac20b3fd",
      "metadata": {
        "id": "ac20b3fd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# xDf_PCA.explained_variance_ratio_ contains the explained variance ratio for each principal component\n",
        "variance_ratio = xDf_PCA.explained_variance_ratio_\n",
        "\n",
        "# Calculate cumulative variance ratio\n",
        "cumulative_var_ratio = np.cumsum(variance_ratio)\n",
        "\n",
        "# Create bar plot with cumulative variance ratio\n",
        "plt.bar(range(1, len(cumulative_var_ratio)+1), cumulative_var_ratio, align='center')\n",
        "plt.xticks(range(1, len(cumulative_var_ratio)+1))\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d164ef1b",
      "metadata": {
        "id": "d164ef1b"
      },
      "source": [
        "### Limitations of PCA\n",
        "1. _Linearity assumption:_ PCA assumes that the data is linearly related. If the data has a nonlinear structure, PCA may not be the most appropriate technique.\n",
        "\n",
        "2. _Loss of interpretability: After performing PCA, the principal components may not be directly interpretable in terms of the original variables. This can make it difficult to explain the results to non-technical stakeholders.\n",
        "\n",
        "3. _Sensitivity to outliers:_ PCA is sensitive to outliers, which can distort the results and lead to incorrect conclusions.\n",
        "\n",
        "4. _Sensitivity to scaling:_ PCA is sensitive to the scale of the variables, which can affect the results. It is important to standardize the variables before performing PCA.\n",
        "\n",
        "5. _Difficulty in choosing the number of components:_ Choosing the number of components to retain can be a challenging task. If too few components are retained, important information may be lost. If too many components are retained, the results may be overfit and not generalize well to new data.\n",
        "\n",
        "6. _Correlation-based:_ PCA assumes that variables are linearly correlated with each other. If the variables are not correlated, PCA may not be the most appropriate technique.\n",
        "\n",
        "7. _Lack of robustness:_ PCA is not a robust technique and can be affected by outliers and influential observations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oHVRlhmPy9VL",
      "metadata": {
        "id": "oHVRlhmPy9VL"
      },
      "source": [
        "## **Image Compression with SVD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05dlXIr2Rla9",
      "metadata": {
        "id": "05dlXIr2Rla9"
      },
      "outputs": [],
      "source": [
        "# import module\n",
        "import requests\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# assign and open image\n",
        "url = 'https://media.geeksforgeeks.org/wp-content/cdn-uploads/20210401173418/Webp-compressed.jpg'\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "with open('image.png', 'wb') as f:\n",
        "\tf.write(response.content)\n",
        "\n",
        "img = cv2.imread('image.png')\n",
        "\n",
        "# Converting the image into gray scale for faster\n",
        "# computation.\n",
        "gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Calculating the SVD\n",
        "u, s, v = np.linalg.svd(gray_image, full_matrices=False)\n",
        "\n",
        "# inspect shapes of the matrices\n",
        "print(f'u.shape:{u.shape},s.shape:{s.shape},v.shape:{v.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QtGbu6Jr7akR",
      "metadata": {
        "id": "QtGbu6Jr7akR"
      },
      "outputs": [],
      "source": [
        "# import module\n",
        "import seaborn as sns\n",
        "\n",
        "var_explained = np.round(s**2/np.sum(s**2), decimals=6)\n",
        "\n",
        "# Variance explained top Singular vectors\n",
        "print(f'variance Explained by Top 20 singular values:\\n{var_explained[0:20]}')\n",
        "\n",
        "sns.barplot(x=list(range(1, 21)),\n",
        "\t\t\ty=var_explained[0:20], color=\"dodgerblue\")\n",
        "\n",
        "plt.title('Variance Explained Graph')\n",
        "plt.xlabel('Singular Vector', fontsize=16)\n",
        "plt.ylabel('Variance Explained', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CbYF-gT_7gpT",
      "metadata": {
        "id": "CbYF-gT_7gpT"
      },
      "outputs": [],
      "source": [
        "# plot images with different number of components\n",
        "comps = [3648, 1, 5, 10, 15, 20]\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for i in range(len(comps)):\n",
        "\tlow_rank = u[:, :comps[i]] @ np.diag(s[:comps[i]]) @ v[:comps[i], :]\n",
        "\n",
        "\tif(i == 0):\n",
        "\t\tplt.subplot(2, 3, i+1),\n",
        "\t\tplt.imshow(low_rank, cmap='gray'),\n",
        "\t\tplt.title(f'Actual Image with n_components = {comps[i]}')\n",
        "\n",
        "\telse:\n",
        "\t\tplt.subplot(2, 3, i+1),\n",
        "\t\tplt.imshow(low_rank, cmap='gray'),\n",
        "\t\tplt.title(f'n_components = {comps[i]}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}